{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df49976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# import logging  # À voir si on l'ajoute pour mieux structurer les messages dans la console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier dataset\n",
    "\n",
    "dataset = \"data/Teacher/NSL_KDD.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ec333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 - Renommer les colonnes du dataset\n",
    "\n",
    "## Liste des nouveaux noms de colonnes\n",
    "\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "\n",
    "## Modification des noms de colonnes\n",
    "dataset = pd.read_csv(dataset, header=None, names=col_names)\n",
    "\n",
    "## Affichage du dataset d'entraînement avec les nouveaux noms de colonnes\n",
    "print(\"Dataset avec les nouveaux noms de colonnes :\")\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 - Visualiser les valeurs de la colonne 'label' et tracer leur répartition\n",
    "\n",
    "colonne_label = dataset['label'] # Sélection de la colonne 'label' du DataFrame\n",
    "\n",
    "## Suppression des valeurs 'normal'\n",
    "colonne_label = colonne_label[colonne_label != \"normal\"]\n",
    "\n",
    "ax = colonne_label.value_counts().plot(\n",
    "    kind='bar',\n",
    "    title='Répartition des types d\\'attaques',\n",
    "    figsize=(10, 6),\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Groupe d\\'attaque')\n",
    "ax.set_ylabel(\"Nombre d'occurrences\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "## Ajouter les valeurs au-dessus des barres\n",
    "for p in ax.patches:\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,   # position horizontale (au centre de la barre)\n",
    "        p.get_height() + 0.5,            # position verticale (légèrement au-dessus)\n",
    "        int(p.get_height()),             # valeur affichée\n",
    "        ha='center',                     # centrer horizontalement\n",
    "        va='bottom'                      # aligner en bas\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5400118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 - Analyse des valeurs manquantes avec une heatmap\n",
    "\n",
    "# Calcul des valeurs manquantes dans le dataset d'entraînement\n",
    "missing_train = dataset.isnull().sum()\n",
    "\n",
    "# Créer et afficher une carte de chaleur pour visualiser les valeurs manquantes\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    dataset.isnull(),\n",
    "    cbar=True,\n",
    "    cmap='viridis',\n",
    "    cbar_kws={'label': 'Valeur manquante (True/False)'}\n",
    ")\n",
    "plt.title(\"Valeurs manquantes dans le dataset\")\n",
    "plt.xlabel('Colonnes')\n",
    "plt.ylabel('Index des lignes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5514c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 - Vérifier le nombre de lignes dupliquées dans le dataset d'entraînement\n",
    "\n",
    "print(\"Nombre total de lignes dupliquées dans le dataset d'entraînement :\", dataset.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6 - Suppression des doublons\n",
    "\n",
    "print(\"Avant suppression, nombre de lignes dans le dataset d'entraînement :\", len(dataset))\n",
    "dataset.drop_duplicates(inplace=True) # Supprimer les doublons\n",
    "print(\"Après suppression, nombre de lignes dans le dataset d'entraînement :\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73778e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10 - Reprendre le dataset d'origine (avant autres traitements) pour la question 10\n",
    "\n",
    "train_data_q10 = dataset.copy() # Faire une copie pour ne pas modifier l'original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7 - Encodage des variables catégorielles\n",
    "\n",
    "## Identifier les colonnes catégorielles\n",
    "cat_columns = dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Colonnes catégorielles trouvées :\", cat_columns)\n",
    "\n",
    "## Afficher les colonnes des valeurs catégorielles\n",
    "for col in cat_columns:\n",
    "    print(f\"\\nColonne : {col}\")\n",
    "    print(dataset[col].value_counts())  # Affiche les valeurs uniques avec leur nombre\n",
    "\n",
    "## Séparer les colonnes selon leur nombre de valeurs uniques\n",
    "cat_columns_few_values = []  # Colonnes avec moins de 5 valeurs uniques\n",
    "cat_columns_many_values = []  # Colonnes avec 5 valeurs uniques ou plus\n",
    "\n",
    "for col in cat_columns:\n",
    "    unique_count = dataset[col].nunique()\n",
    "    if unique_count < 5:\n",
    "        cat_columns_few_values.append(col)\n",
    "    else:\n",
    "        cat_columns_many_values.append(col)\n",
    "\n",
    "print(f\"\\nColonnes catégorielles avec moins de 5 valeurs uniques : {cat_columns_few_values}\")\n",
    "print(f\"Colonnes catégorielles avec 5 valeurs uniques ou plus : {cat_columns_many_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52616b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7 (suite) - Encodage des variables catégorielles\n",
    "\n",
    "print(f\"Nombre de colonnes avant encodage : {len(dataset.columns)}\")\n",
    "\n",
    "\n",
    "## Encoder chaque colonne catégorielle avec LabelEncoder\n",
    "for col in cat_columns_many_values:\n",
    "    dataset[col] = LabelEncoder().fit_transform(dataset[col])\n",
    "\n",
    "\n",
    "\n",
    "## Encoder chaque colonne catégorielle avec OneHotEncoder\n",
    "enc = OneHotEncoder(sparse_output=False, dtype='int')  # dtype='int' pour avoir des entiers au lieu de float\n",
    "\n",
    "### Traiter chaque colonne catégorielle une par une avec une boucle\n",
    "for col in cat_columns_few_values:\n",
    "    print(f\"Traitement de la colonne : {col}\")\n",
    "    \n",
    "    \n",
    "    # Préparer les données pour OneHotEncoder (reshape en 2D)\n",
    "    X = dataset[col].values.reshape(-1, 1)\n",
    "    \n",
    "    # Encoder la colonne\n",
    "    encoded_data = enc.fit_transform(X)\n",
    "    \n",
    "    # Créer les noms des nouvelles colonnes\n",
    "    feature_names = enc.get_feature_names_out([col])\n",
    "    \n",
    "    # Créer un DataFrame avec les données encodées\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=feature_names, index=dataset.index)\n",
    "    \n",
    "    # Ajouter les nouvelles colonnes au dataset\n",
    "    dataset = pd.concat([dataset, encoded_df], axis=1)\n",
    "    \n",
    "    # Supprimer l'ancienne colonne du dataset\n",
    "    dataset = dataset.drop(columns=[col])\n",
    "    \n",
    "    print(f\"Colonnes ajoutées: {list(feature_names)}\")\n",
    "    print(f\"Colonne {col} supprimée\\n\")\n",
    "\n",
    "print(f\"Nombre de colonnes après encodage : {len(dataset.columns)}\")\n",
    "\n",
    "## Sauvegarder les noms des colonnes après encodage\n",
    "col_names_encoded = list(dataset.columns)\n",
    "print(f\"Noms des colonnes sauvegardés : {col_names_encoded}\")\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac51333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Question 8 - Normalisation / Mise à l'échelle\n",
    "\n",
    "## Copy du dataset pour éviter de modifier l'original\n",
    "dataset_StandardScaler = dataset.copy()\n",
    "dataset_MinMaxScaler = dataset.copy()\n",
    "dataset_RobustScaler = dataset.copy()\n",
    "\n",
    "## Afficher les valeurs aberrantes avant normalisation\n",
    "sns.boxplot(data=dataset)\n",
    "plt.title(\"Valeurs aberrantes avant normalisation\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Appliquer RobustScaler\n",
    "dataset_RobustScaler = RobustScaler().fit_transform(dataset_RobustScaler)\n",
    "\n",
    "sns.boxplot(data=dataset_RobustScaler)\n",
    "plt.title(\"Valeurs aberrantes après normalisation RobustScaler\")\n",
    "plt.show()\n",
    "\n",
    "## Appliquer MinMaxScaler\n",
    "dataset_MinMaxScaler = MinMaxScaler().fit_transform(dataset_MinMaxScaler)\n",
    "\n",
    "sns.boxplot(data=dataset_MinMaxScaler)\n",
    "plt.title(\"Valeurs aberrantes après normalisation MinMaxScaler\")\n",
    "plt.show()\n",
    "\n",
    "## Appliquer StandardScaler\n",
    "dataset_StandardScaler = StandardScaler().fit_transform(dataset_StandardScaler)\n",
    "sns.boxplot(data=dataset_StandardScaler)\n",
    "plt.title(\"Valeurs aberrantes après normalisation StandardScaler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8 (suite) - Normalisation / Mise à l'échelle\n",
    "\n",
    "## Utilisation de la fonction StandardScaler\n",
    "dataset = StandardScaler().fit_transform(dataset)\n",
    "\n",
    "# Reconvertir l'array NumPy en DataFrame pandas (nécessaire après StandardScaler)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "## Réassigner les noms de colonnes au DataFrame\n",
    "dataset.columns = col_names_encoded\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec91d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9 - Découpage en ensembles (train / test)\n",
    "\n",
    "# Séparation des features (X) et du target (y)\n",
    "X = dataset.drop('label', axis=1)  # Toutes les colonnes sauf 'label'\n",
    "y = dataset['label']  # Seulement la colonne 'label'\n",
    "\n",
    "# Découpage train/test (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## Affichage du nombre de lignes dans chaque jeu\n",
    "print(\"Nombre de lignes dans le jeu d'entraînement final :\", len(X_train))\n",
    "print(\"Nombre de lignes dans le jeu de test final :\", len(X_test))\n",
    "\n",
    "## Afficher les tailles et la répartition des classes\n",
    "print(f\"\\nTaille des ensembles:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "## Sauvegarde des fichiers en format CSV (Optionnel)\n",
    "out_dir = 'data/Dataset_pre_traiter/first_part'\n",
    "\n",
    "# Sauvegarder chaque objet (features et target) dans des fichiers séparés\n",
    "X_train.to_csv(f'{out_dir}/X_train.csv', index=False)\n",
    "y_train.to_csv(f'{out_dir}/y_train.csv', index=False)\n",
    "X_test.to_csv(f'{out_dir}/X_test.csv', index=False)\n",
    "y_test.to_csv(f'{out_dir}/y_test.csv', index=False)\n",
    "\n",
    "print(f'Données sauvegardées dans le dossier : {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10 (suite) - Regroupement des attaques en grandes familles (DOS, Probe, R2L, U2R)\n",
    "\n",
    "## Listes fournies stockées dans des variables\n",
    "dos = [\n",
    "    \"back\",\"land\",\"neptune\",\"pod\",\"smurf\",\"teardrop\",\"mailbomb\",\n",
    "    \"processtable\",\"udpstorm\",\"apache2\",\"worm\"\n",
    "]\n",
    "\n",
    "probe = [\n",
    "    \"satan\",\"ipsweep\",\"nmap\",\"portsweep\",\"mscan\",\"saint\"\n",
    "]\n",
    "\n",
    "r2l = [\n",
    "    \"guess_passwd\",\"ftp_write\",\"imap\",\"phf\",\"multihop\",\"warezmaster\",\n",
    "    \"xlock\",\"xsnoop\",\"snmpguess\",\"snmpgetattack\",\"httptunnel\",\"sendmail\",\"named\", \"warezclient\"\n",
    "]\n",
    "\n",
    "u2r = [\n",
    "    \"buffer_overflow\",\"loadmodule\",\"rootkit\",\"perl\",\"sqlattack\",\"xterm\",\"ps\"\n",
    "]\n",
    "\n",
    "normal = [\"normal\"]\n",
    "\n",
    "## Construire un dictionnaire de correspondance (label -> grand groupe)\n",
    "mapping = {}\n",
    "for name in dos:\n",
    "    mapping[name] = 'DOS'\n",
    "for name in probe:\n",
    "    mapping[name] = 'Probe'\n",
    "for name in r2l:\n",
    "    mapping[name] = 'R2L'\n",
    "for name in u2r:\n",
    "    mapping[name] = 'U2R'\n",
    "for name in normal:\n",
    "    mapping[name] = 'Normal'\n",
    "\n",
    "## Lecture / extraction de la colonne 'label'\n",
    "labels_raw = train_data_q10['label'].astype(str)  # S'assurer que la colonne est du texte\n",
    "\n",
    "## Mettre 'Other' pour les labels inconnus (non présents dans les listes fournies)\n",
    "labels_grouped = labels_raw.apply(lambda x: mapping.get(x, 'Other')) \n",
    "\n",
    "## Afficher les comptes par grand groupe\n",
    "print(\"Comptes par grand groupe :\")\n",
    "print(labels_grouped.value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "## Ajouter la nouvelle colonne au jeu d'entraînement et afficher le résultat\n",
    "train_data_q10['attack_family'] = labels_grouped\n",
    "print(\"Jeu d'entraînement avec la nouvelle colonne 'attack_family' :\")\n",
    "display(train_data_q10)\n",
    "print(\"\\n\")\n",
    "print(\"Voir les lignes ou il y a la valeur 'Other' dans la colonne 'attack_family' :\")\n",
    "display(train_data_q10[train_data_q10['attack_family'] == 'Other'])\n",
    "print(\"\\n\")\n",
    "\n",
    "## Découpage par famille d'attaque en jeux distincts\n",
    "\n",
    "dataset_DOS = train_data_q10[train_data_q10['attack_family'] == 'DOS']\n",
    "dataset_Probe = train_data_q10[train_data_q10['attack_family'] == 'Probe']\n",
    "dataset_R2L = train_data_q10[train_data_q10['attack_family'] == 'R2L']\n",
    "dataset_U2R = train_data_q10[train_data_q10['attack_family'] == 'U2R']\n",
    "dataset_Normal = train_data_q10[train_data_q10['attack_family'] == 'Normal']\n",
    "\n",
    "## Liste de tous les jeux pour des boucles ultérieures\n",
    "list_datasets = [dataset_DOS, dataset_Probe, dataset_R2L, dataset_U2R, dataset_Normal]\n",
    "list_names = ['DOS', 'Probe', 'R2L', 'U2R', 'Normal']\n",
    "\n",
    "## Vérification des jeux créés (boucle où \"i\" est l'index et \"dataset\" est la variable concrète)\n",
    "for i, dataset in enumerate(list_datasets):\n",
    "    print(f\"Affichage du dataset {list_names[i]} : \")\n",
    "    display(dataset)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10 (suite) - Encodage des variables catégorielles\n",
    "\n",
    "## Pour éviter les warnings de pandas lors de la modification de copies de DataFrames\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "\n",
    "for dataset in list_datasets:\n",
    "\n",
    "    ## Identifier les colonnes catégorielles\n",
    "    cat_columns = dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(\"Colonnes catégorielles trouvées :\", cat_columns)\n",
    "\n",
    "    ## Encoder chaque colonne catégorielle que j'avais repérée\n",
    "    le = LabelEncoder()  # Utilisation de LabelEncoder\n",
    "    for col in cat_columns:\n",
    "        dataset[col] = le.fit_transform(dataset[col])\n",
    "\n",
    "    display(dataset)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e458c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10 (suite) - Normalisation / Mise à l'échelle\n",
    "\n",
    "for i, dataset in enumerate(list_datasets):\n",
    "    print(f\"Traitement du dataset {list_names[i]} : \")\n",
    "    ## Utilisation de la fonction StandardScaler\n",
    "    dataset = StandardScaler().fit_transform(dataset)\n",
    "\n",
    "    ## Reconvertir l'array NumPy en DataFrame pandas (nécessaire après StandardScaler)\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "\n",
    "    ## Liste des nouveaux noms de colonnes\n",
    "\n",
    "    col_names_new = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"attack_family\"]\n",
    "\n",
    "    ## Réassigner les noms de colonnes au DataFrame\n",
    "    dataset.columns = col_names_new\n",
    "\n",
    "    display(dataset)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10 (suite) - Découpage en ensembles (train / test)\n",
    "\n",
    "for i, dataset in enumerate(list_datasets):\n",
    "\n",
    "    ## Suppression de la colonne \"attack_family\" car elle reste à zéro\n",
    "    dataset = dataset.drop(columns=['attack_family'])\n",
    "\n",
    "    # Séparation des features (X) et du target (y)\n",
    "    X = dataset.drop('label', axis=1)  # Toutes les colonnes sauf 'label'\n",
    "    y = dataset['label']  # Seulement la colonne 'label'\n",
    "\n",
    "    # Découpage train/test (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    ## Affichage du nombre de lignes dans chaque jeu\n",
    "    print(f\"Dataset {list_names[i]} :\")\n",
    "    print(\"Nombre de lignes dans le jeu d'entraînement final :\", len(X_train))\n",
    "    print(\"Nombre de lignes dans le jeu de test final :\", len(X_test))\n",
    "\n",
    "    ## Afficher les tailles et la répartition des classes\n",
    "    print(f\"\\nTaille des ensembles:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    print(f\"\\nRépartition des classes dans l'ensemble d'entraînement:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    print(f\"\\nRépartition des classes dans l'ensemble de test:\")\n",
    "    print(y_test.value_counts())\n",
    "\n",
    "    ## Sauvegarde des fichiers en format CSV (Optionnel)\n",
    "    ### Reconstruction des DataFrames complets pour la sauvegarde\n",
    "    train_data_final = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data_final = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    ### Sauvegarde\n",
    "    train_data_final.to_csv(f\"data/Dataset_pre_traiter/NSL_KDD_train_final_{list_names[i]}.csv\", index=False)\n",
    "    test_data_final.to_csv(f\"data/Dataset_pre_traiter/NSL_KDD_test_final_{list_names[i]}.csv\", index=False)\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
